{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise - 12\nImplement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:12.523953Z","iopub.execute_input":"2024-09-22T07:07:12.524442Z","iopub.status.idle":"2024-09-22T07:07:14.148685Z","shell.execute_reply.started":"2024-09-22T07:07:12.524393Z","shell.execute_reply":"2024-09-22T07:07:14.147368Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"iris = load_iris()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.151601Z","iopub.execute_input":"2024-09-22T07:07:14.152284Z","iopub.status.idle":"2024-09-22T07:07:14.165615Z","shell.execute_reply.started":"2024-09-22T07:07:14.152227Z","shell.execute_reply":"2024-09-22T07:07:14.164316Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"X, y = iris.data, iris.target","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.171166Z","iopub.execute_input":"2024-09-22T07:07:14.172055Z","iopub.status.idle":"2024-09-22T07:07:14.182714Z","shell.execute_reply.started":"2024-09-22T07:07:14.171997Z","shell.execute_reply":"2024-09-22T07:07:14.181176Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.185415Z","iopub.execute_input":"2024-09-22T07:07:14.186737Z","iopub.status.idle":"2024-09-22T07:07:14.203691Z","shell.execute_reply.started":"2024-09-22T07:07:14.186600Z","shell.execute_reply":"2024-09-22T07:07:14.202361Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2]])"},"metadata":{}}]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.204952Z","iopub.execute_input":"2024-09-22T07:07:14.205427Z","iopub.status.idle":"2024-09-22T07:07:14.218427Z","shell.execute_reply.started":"2024-09-22T07:07:14.205389Z","shell.execute_reply":"2024-09-22T07:07:14.217080Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Adding bias term","metadata":{}},{"cell_type":"code","source":"X_with_bias = np.column_stack((np.array([1] * len(X)), X))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.220232Z","iopub.execute_input":"2024-09-22T07:07:14.221188Z","iopub.status.idle":"2024-09-22T07:07:14.228850Z","shell.execute_reply.started":"2024-09-22T07:07:14.221085Z","shell.execute_reply":"2024-09-22T07:07:14.227567Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_with_bias[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.230271Z","iopub.execute_input":"2024-09-22T07:07:14.230906Z","iopub.status.idle":"2024-09-22T07:07:14.246718Z","shell.execute_reply.started":"2024-09-22T07:07:14.230783Z","shell.execute_reply":"2024-09-22T07:07:14.245627Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[1. , 5.1, 3.5, 1.4, 0.2],\n       [1. , 4.9, 3. , 1.4, 0.2],\n       [1. , 4.7, 3.2, 1.3, 0.2],\n       [1. , 4.6, 3.1, 1.5, 0.2],\n       [1. , 5. , 3.6, 1.4, 0.2]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Splitting dataset to train and test sets","metadata":{}},{"cell_type":"code","source":"def train_valid_test_split(\n    X: np.ndarray, \n    y: np.ndarray, \n    *,\n    ratio: float = 0.2, \n    random_state: int | None = None\n) -> tuple[np.ndarray, ...]:     # represents multiple np.ndarray in tuple\n    \"\"\"\n    Splits the dataset into X_train, X_valid, X_test, y_train, y_valid, y_test. Also shuffles the data.\n    \n    Parameters:\n        - X (np.ndarray): NumPy ndarray of features.\n        - y (np.ndarray): NumPy ndarray of labels / target.\n        - ratio (float): Ratio of dataset to split into validation set and test set. Default 0.2, i.e. 20%-20% data is used for validation and test set.\n        - random_state (int): Uses a specific seed for randomness if provided, default None.\n    \n    Returns:\n        - tuple[np.ndarray, ...]: Tuple of 6 NumPy ndarray in order of X_train, X_valid, X_test, y_train, y_valid, y_test.\n    \"\"\"\n    if random_state:\n        np.random.seed(random_state)\n        \n    total_size = len(X)\n    test_size = valid_size = int(total_size * ratio)\n    train_size = total_size - test_size - valid_size\n    \n    rnd_indices = np.random.permutation(total_size)\n    \n    X_train = X[rnd_indices[:train_size]]\n    y_train = y[rnd_indices[:train_size]]\n    \n    X_valid = X[rnd_indices[train_size: -test_size]]\n    y_valid = y[rnd_indices[train_size: -test_size]]\n    \n    X_test = X[rnd_indices[-test_size:]]\n    y_test = y[rnd_indices[-test_size:]]\n    \n    return X_train, X_valid, X_test, y_train, y_valid, y_test","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.248286Z","iopub.execute_input":"2024-09-22T07:07:14.249240Z","iopub.status.idle":"2024-09-22T07:07:14.260404Z","shell.execute_reply.started":"2024-09-22T07:07:14.249197Z","shell.execute_reply":"2024-09-22T07:07:14.259179Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X_with_bias, y, random_state= 42)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.261979Z","iopub.execute_input":"2024-09-22T07:07:14.262960Z","iopub.status.idle":"2024-09-22T07:07:14.273895Z","shell.execute_reply.started":"2024-09-22T07:07:14.262910Z","shell.execute_reply":"2024-09-22T07:07:14.272609Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"list(map(len, (X_train, X_valid, X_test, y_train, y_valid, y_test)))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.280753Z","iopub.execute_input":"2024-09-22T07:07:14.281207Z","iopub.status.idle":"2024-09-22T07:07:14.290271Z","shell.execute_reply.started":"2024-09-22T07:07:14.281166Z","shell.execute_reply":"2024-09-22T07:07:14.289050Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[90, 30, 30, 90, 30, 30]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Transforming labels to one - hot vector","metadata":{}},{"cell_type":"code","source":"def one_hot_vector(y: np.ndarray) -> np.ndarray:\n    return np.diag(np.ones(y.max() + 1))[y]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.291341Z","iopub.execute_input":"2024-09-22T07:07:14.291727Z","iopub.status.idle":"2024-09-22T07:07:14.301841Z","shell.execute_reply.started":"2024-09-22T07:07:14.291679Z","shell.execute_reply":"2024-09-22T07:07:14.300554Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"one_hot_vector(y_train[:10])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.303288Z","iopub.execute_input":"2024-09-22T07:07:14.304489Z","iopub.status.idle":"2024-09-22T07:07:14.319439Z","shell.execute_reply.started":"2024-09-22T07:07:14.304433Z","shell.execute_reply":"2024-09-22T07:07:14.318184Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"y_train_1_hot = one_hot_vector(y_train)\ny_valid_1_hot = one_hot_vector(y_valid)\ny_test_1_hot = one_hot_vector(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.321206Z","iopub.execute_input":"2024-09-22T07:07:14.321721Z","iopub.status.idle":"2024-09-22T07:07:14.334270Z","shell.execute_reply.started":"2024-09-22T07:07:14.321652Z","shell.execute_reply":"2024-09-22T07:07:14.332709Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Scaling Data","metadata":{}},{"cell_type":"code","source":"mean = X_train[:, 1:].mean(axis= 0)\nstd = X_train[:, 1:].std(axis= 0)\n\nX_train[:, 1:] = (X_train[:, 1:] - mean) / std\nX_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\nX_test[:, 1:] = (X_test[:, 1:] - mean) / std","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.335941Z","iopub.execute_input":"2024-09-22T07:07:14.336502Z","iopub.status.idle":"2024-09-22T07:07:14.351476Z","shell.execute_reply.started":"2024-09-22T07:07:14.336449Z","shell.execute_reply":"2024-09-22T07:07:14.350264Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.353443Z","iopub.execute_input":"2024-09-22T07:07:14.353972Z","iopub.status.idle":"2024-09-22T07:07:14.366732Z","shell.execute_reply.started":"2024-09-22T07:07:14.353885Z","shell.execute_reply":"2024-09-22T07:07:14.365422Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([ 1.        ,  0.39652122, -0.65957023,  0.63935691,  0.10418645])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Softmax Function\nImplementing the softmax formula:\n\n$\\hat{\\mathbf{p}_k} = \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$","metadata":{}},{"cell_type":"code","source":"def softmax(logits: np.ndarray) -> np.ndarray:\n    exps = np.exp(logits)\n    exps_sum = np.sum(exps, axis= 1, keepdims= True)   # keepdims returns the result as array\n    return exps / exps_sum","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.368357Z","iopub.execute_input":"2024-09-22T07:07:14.368850Z","iopub.status.idle":"2024-09-22T07:07:14.376342Z","shell.execute_reply.started":"2024-09-22T07:07:14.368800Z","shell.execute_reply":"2024-09-22T07:07:14.375178Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"softmax(y_train_1_hot[:10])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.377952Z","iopub.execute_input":"2024-09-22T07:07:14.378518Z","iopub.status.idle":"2024-09-22T07:07:14.390433Z","shell.execute_reply.started":"2024-09-22T07:07:14.378467Z","shell.execute_reply":"2024-09-22T07:07:14.389168Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([[0.21194156, 0.57611688, 0.21194156],\n       [0.57611688, 0.21194156, 0.21194156],\n       [0.21194156, 0.21194156, 0.57611688],\n       [0.21194156, 0.57611688, 0.21194156],\n       [0.21194156, 0.57611688, 0.21194156],\n       [0.57611688, 0.21194156, 0.21194156],\n       [0.21194156, 0.57611688, 0.21194156],\n       [0.21194156, 0.21194156, 0.57611688],\n       [0.21194156, 0.57611688, 0.21194156],\n       [0.21194156, 0.57611688, 0.21194156]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"n_inputs = X_train.shape[1]\nn_outputs = len(np.unique(y_train))","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.392314Z","iopub.execute_input":"2024-09-22T07:07:14.392771Z","iopub.status.idle":"2024-09-22T07:07:14.402575Z","shell.execute_reply.started":"2024-09-22T07:07:14.392721Z","shell.execute_reply":"2024-09-22T07:07:14.401346Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"n_inputs, n_outputs","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:07:14.403779Z","iopub.execute_input":"2024-09-22T07:07:14.404581Z","iopub.status.idle":"2024-09-22T07:07:14.416004Z","shell.execute_reply.started":"2024-09-22T07:07:14.404537Z","shell.execute_reply":"2024-09-22T07:07:14.414626Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(5, 3)"},"metadata":{}}]},{"cell_type":"code","source":"eta = 0.1         # Learning rate\nn_epochs = 50000  # number of epochs\nm = len(X_train)  # number of training instances\nepsilon = 1e-5    # adding this small value to remove nan values caused by pk hat = 0 while taking log\nalpha = 0.01      # regularization hyperparameter\nbest_loss = float('inf')    # setting initial loss to infinity\n\nnp.random.seed(42)\ntheta = np.random.randn(n_inputs, n_outputs)    # random parameter initialization\n\nfor epoch in range(n_epochs):\n    # getting scores/logits\n    logits = X_train @ theta\n    \n    # using softmax function to get probabilities\n    y_proba = softmax(logits)\n    y_proba_valid = softmax(X_valid @ theta)\n    \n    # calculating cross entropy loss and l2_loss\n    cross_entropy_loss = -(y_valid_1_hot * np.log(y_proba_valid + epsilon)).sum(axis=1).mean()    # J(theta) of softmax regression\n    l2_loss = alpha / m * (theta[1:] ** 2).sum()     # l2 loss of ridge regression, to perform regularization\n    total_loss = cross_entropy_loss + l2_loss\n    \n    # Early Stopping\n    if total_loss < best_loss:\n        best_loss = total_loss\n    else:\n        print(epoch - 1, best_loss)\n        print(epoch, total_loss, \"early stopping!\")\n        break\n        \n    # basic training part, calculating gradients and updating theta\n    error = y_proba - y_train_1_hot\n    gradients = 1 / m * X_train.T @ error\n    theta = theta - eta * gradients","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:10:00.191737Z","iopub.execute_input":"2024-09-22T07:10:00.192291Z","iopub.status.idle":"2024-09-22T07:10:00.944811Z","shell.execute_reply.started":"2024-09-22T07:10:00.192244Z","shell.execute_reply":"2024-09-22T07:10:00.943423Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"12449 0.10838445296778279\n12450 0.10838445296914534 early stopping!\n","output_type":"stream"}]},{"cell_type":"code","source":"theta","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:10:00.946770Z","iopub.execute_input":"2024-09-22T07:10:00.947169Z","iopub.status.idle":"2024-09-22T07:10:00.954990Z","shell.execute_reply.started":"2024-09-22T07:10:00.947125Z","shell.execute_reply":"2024-09-22T07:10:00.953875Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array([[ 0.90418494,  5.17526854, -5.07331508],\n       [-2.13513884,  1.95479478,  1.23508358],\n       [ 3.24204237,  0.05690417, -1.42177338],\n       [-4.37796656, -1.27985305,  5.2712322 ],\n       [-4.61821162, -2.92627525,  4.14825106]])"},"metadata":{}}]},{"cell_type":"code","source":"logits = X_valid @ theta\ny_proba = softmax(logits)\npredictions = y_proba.argmax(axis= 1)\n\naccuracy_score = (predictions == y_valid).mean()\nprint(f'{accuracy_score = :.2%}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:10:00.956457Z","iopub.execute_input":"2024-09-22T07:10:00.956846Z","iopub.status.idle":"2024-09-22T07:10:00.967493Z","shell.execute_reply.started":"2024-09-22T07:10:00.956796Z","shell.execute_reply":"2024-09-22T07:10:00.966158Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"accuracy_score = 96.67%\n","output_type":"stream"}]},{"cell_type":"code","source":"logits = X_test @ theta\ny_proba = softmax(logits)\npredictions = y_proba.argmax(axis= 1)\n\naccuracy_score = (predictions == y_test).mean()\nprint(f'{accuracy_score = :.2%}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T07:10:03.851963Z","iopub.execute_input":"2024-09-22T07:10:03.852377Z","iopub.status.idle":"2024-09-22T07:10:03.859264Z","shell.execute_reply.started":"2024-09-22T07:10:03.852341Z","shell.execute_reply":"2024-09-22T07:10:03.857979Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"accuracy_score = 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Well it was kinda unexpected to get such an insane accuracy, but probably it was because of small size of dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}