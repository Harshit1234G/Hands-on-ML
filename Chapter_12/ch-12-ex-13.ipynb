{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train a model using a custom training loop to tackle the Fashion MNIST dataset\n1. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\n2. Try using a different optimizer with a different learning rate for the upper layers and the lower layers.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from collections import OrderedDict\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.auto import trange","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:31.921214Z","iopub.execute_input":"2026-01-19T08:16:31.921720Z","iopub.status.idle":"2026-01-19T08:16:31.925316Z","shell.execute_reply.started":"2026-01-19T08:16:31.921693Z","shell.execute_reply":"2026-01-19T08:16:31.924566Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype('float32') / 255.\nX_valid, X_train = X_train_full[:5000], X_train_full[5000:]\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nX_test = X_test.astype('float32') / 255.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:31.926532Z","iopub.execute_input":"2026-01-19T08:16:31.926791Z","iopub.status.idle":"2026-01-19T08:16:32.863550Z","shell.execute_reply.started":"2026-01-19T08:16:31.926768Z","shell.execute_reply":"2026-01-19T08:16:32.862740Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:32.864508Z","iopub.execute_input":"2026-01-19T08:16:32.864795Z","iopub.status.idle":"2026-01-19T08:16:32.868732Z","shell.execute_reply.started":"2026-01-19T08:16:32.864768Z","shell.execute_reply":"2026-01-19T08:16:32.868147Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"lower_layers = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape= [28, 28]),\n    tf.keras.layers.Dense(100, activation= 'relu'),\n])\nupper_layers = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation= 'softmax'),\n])\nmodel = tf.keras.Sequential([\n    lower_layers, upper_layers\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:32.870406Z","iopub.execute_input":"2026-01-19T08:16:32.870678Z","iopub.status.idle":"2026-01-19T08:16:35.053377Z","shell.execute_reply.started":"2026-01-19T08:16:32.870658Z","shell.execute_reply":"2026-01-19T08:16:35.052637Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\nI0000 00:00:1768810593.539808      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"lower_optimizer = tf.keras.optimizers.SGD(learning_rate= 1e-4)\nupper_optimizer = tf.keras.optimizers.Nadam(learning_rate= 1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:35.054338Z","iopub.execute_input":"2026-01-19T08:16:35.054626Z","iopub.status.idle":"2026-01-19T08:16:35.066890Z","shell.execute_reply.started":"2026-01-19T08:16:35.054597Z","shell.execute_reply":"2026-01-19T08:16:35.066348Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"n_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\nloss_fn = tf.keras.losses.sparse_categorical_crossentropy\nmean_loss = tf.keras.metrics.Mean()\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:35.067845Z","iopub.execute_input":"2026-01-19T08:16:35.068110Z","iopub.status.idle":"2026-01-19T08:16:35.077772Z","shell.execute_reply.started":"2026-01-19T08:16:35.068088Z","shell.execute_reply":"2026-01-19T08:16:35.077219Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def random_batch(X, y, *, batch_size: int= 32) -> tuple:\n    idx = np.random.randint(len(X), size= batch_size)\n    return X[idx], y[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:35.078613Z","iopub.execute_input":"2026-01-19T08:16:35.078844Z","iopub.status.idle":"2026-01-19T08:16:35.082803Z","shell.execute_reply.started":"2026-01-19T08:16:35.078824Z","shell.execute_reply":"2026-01-19T08:16:35.082245Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"with trange(1, n_epochs + 1, desc= 'All epochs') as epochs:\n    for epoch in epochs:\n        # Progress bar for steps within the current epoch\n        with trange(1, n_steps + 1, desc= f'Epoch {epoch}/{n_epochs}', leave= False) as steps:\n            for step in steps:\n                # --------------------------------------------------\n                # 1. Sample a random mini-batch\n                # --------------------------------------------------\n                X_batch, y_batch = random_batch(X_train, y_train)\n\n                # --------------------------------------------------\n                # 2. Forward pass + loss computation\n                # --------------------------------------------------\n                with tf.GradientTape(persistent= True) as tape:\n                    y_pred = model(X_batch)\n                    # Primary task loss\n                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n                    # Add regularization losses (e.g., L2)\n                    loss = tf.add_n([main_loss] + model.losses)\n\n                # --------------------------------------------------\n                # 3. Backpropagation with different optimizers\n                # --------------------------------------------------\n                for layers, optimizer in (\n                    (lower_layers, lower_optimizer),\n                    (upper_layers, upper_optimizer),\n                ):\n                    grads = tape.gradient(loss, layers.trainable_variables)\n                    # Filter out None gradients (safety)\n                    grads_vars = [\n                        (g, v) for g, v in zip(grads, layers.trainable_variables)\n                        if g is not None\n                    ]\n                    optimizer.apply_gradients(grads_vars)\n\n                # Explicitly release resources\n                del tape\n\n                # --------------------------------------------------\n                # 4. Apply variable constraints (if any)\n                # --------------------------------------------------\n                for var in model.variables:\n                    if var.constraint is not None:\n                        var.assign(var.constraint(var))\n\n                # --------------------------------------------------\n                # 5. Update training metrics\n                # --------------------------------------------------\n                status = OrderedDict()\n\n                mean_loss.update_state(loss)\n                status['loss'] = mean_loss.result().numpy()\n\n                for metric in metrics:\n                    metric.update_state(y_batch, y_pred)\n                    status[metric.name] = metric.result().numpy()\n\n                # Update tqdm display\n                steps.set_postfix(status)\n\n        # ------------------------------------------------------\n        # 6. Validation phase (outside step loop)\n        # ------------------------------------------------------\n        y_val_pred = model(X_valid, training= False)\n\n        val_loss = tf.reduce_mean(loss_fn(y_valid, y_val_pred))\n        val_acc = tf.reduce_mean(\n            tf.keras.metrics.sparse_categorical_accuracy(y_valid, y_val_pred)\n        )\n\n        epochs.set_postfix(\n            {\n                'loss': status['loss'],\n                'val_loss': val_loss.numpy(),\n                'val_accuracy': val_acc.numpy(),\n            }\n        )\n\n        # ------------------------------------------------------\n        # 7. Reset metrics at end of epoch\n        # ------------------------------------------------------\n        for metric in [mean_loss] + metrics:\n            metric.reset_state()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T08:16:35.083684Z","iopub.execute_input":"2026-01-19T08:16:35.084111Z","iopub.status.idle":"2026-01-19T08:21:09.195494Z","shell.execute_reply.started":"2026-01-19T08:16:35.084091Z","shell.execute_reply":"2026-01-19T08:21:09.194661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"All epochs:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b332410a9a0f4b1aacc840e49c0de739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/1718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/1718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/1718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/1718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/1718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}